# -*- coding: utf-8 -*-
"""supportticketssearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7xdpwDej0zjK0MyCYku-j_g9Bvcd4dl
"""
import sys
import pysqlite3
sys.modules["sqlite3"] = pysqlite3
sys.modules["pysqlite3"] = pysqlite3

import pandas as pd
import streamlit as st
import chromadb
import openai
from openai import OpenAI
import streamlit as st
import os
import heapq
from collections import defaultdict
import requests
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch





github_url = "https://raw.githubusercontent.com/pqhunter15/supportticketchromasimilarity/main/support_cleaned_1.csv"
df = pd.read_csv(github_url)

from chroma_setup import load_chroma_collection


# Only load once per session
if "collection" not in st.session_state:
    st.session_state.collection = load_chroma_collection()

collection = st.session_state.collection

query = st.text_input("Enter your question:")


#query rewriting aggregation

import requests
import streamlit as st

# Load model and tokenizer
@st.cache_resource
def load_t5_model():
    tokenizer = AutoTokenizer.from_pretrained("t5-small", use_fast=False)
    model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return tokenizer, model, device

tokenizer, model, device = load_t5_model()

def rewrite_query_local(original_query, num_rewrites=2):
    reworded = []
    input_text = f"paraphrase: {original_query} </s>"

    encoding = tokenizer.encode_plus(
        input_text,
        max_length=256,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    ).to(device)

    for _ in range(num_rewrites):
        output = model.generate(
            input_ids=encoding["input_ids"],
            attention_mask=encoding["attention_mask"],
            max_length=256,
            num_beams=5,
            do_sample=True,
            temperature=1.2,
            num_return_sequences=1,
        )

        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        reworded.append(decoded)

    return reworded


top_k = 3

# Add tag filters in the sidebar
st.sidebar.header("Optional Tag Filters")
tag1 = st.sidebar.text_input("Filter Tag 1")
tag2 = st.sidebar.text_input("Filter Tag 2")
tag3 = st.sidebar.text_input("Filter Tag 3")

# Build metadata filtering logic
filters = []
for tag in [tag1, tag2, tag3]:
    if tag:
        filters.append({
            "$or": [
                {"tech_tag_1": tag},
                {"tech_tag_2": tag},
                {"tech_tag_3": tag}
            ]
        })

# Combine filters into a single where clause
# Safely construct the where clause
if len(filters) == 1:
    where_clause = filters[0]  # single OR block
elif len(filters) > 1:
    where_clause = {"$and": filters}  # multiple filters combined with AND
else:
    where_clause = None  # no filters

# Query Chroma collection with optional filters
if query and "collection" in st.session_state:
    # Step 1: Rewrite queries
    reworded_queries = rewrite_query_local(query, num_rewrites=2)
    all_queries = [query] + reworded_queries

    # Step 2: Query Chroma for each rewritten version
    top_k = 3
    results_by_doc = {}

    for q in all_queries:
        results = st.session_state.collection.query(
            query_texts=[q],
            n_results=top_k,
            where=where_clause if where_clause else None,  # âœ… include metadata filter
            include=["documents", "metadatas", "distances"]
        )

        # Step 3: Aggregate by doc_id with best (lowest) distance
        for doc, meta, dist in zip(*[results[k][0] for k in ["documents", "metadatas", "distances"]]):
            doc_id = int(meta["original_doc_id"])
            if doc_id not in results_by_doc or dist < results_by_doc[doc_id]["dist"]:
                results_by_doc[doc_id] = {
                    "meta": meta,
                    "doc": doc,
                    "dist": dist
                }

    # Step 4: Sort aggregated results by cosine distance
    sorted_results = sorted(results_by_doc.items(), key=lambda x: x[1]["dist"])
    top_results = sorted_results[:top_k]

    # Step 5: Display results
    st.markdown("### Results:")
    for doc_id, entry in top_results:
        match_row = df[df["doc_id"] == doc_id]
        if not match_row.empty:
            row = match_row.iloc[0]
            st.markdown(f"""
            **Doc ID:** {doc_id}  
            **Similarity:** {entry['dist']:.4f}  
            **Topic:** {row['topic_label']}  
            **Answer:** {row['answer']}  
            **Body:** {row['body'][:300]}...
            """)





